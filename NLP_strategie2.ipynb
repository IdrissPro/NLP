{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Stratégie 2"
      ],
      "metadata": {
        "id": "4handa_9_ruK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ici on s'occupe de charger toutes les bibliothèques nécessaire pour notre étude."
      ],
      "metadata": {
        "id": "XfQOhmyUUq-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import sklearn as skl\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "RIb3C3Lq_tQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On commence par afficher un premier apperçu de notre dataset, on remarque que le jeu de données contient des informations journalières de 2017 à 2023 sur 8 entreprises du CAC 40. Pour chaque jour, il indique s’il y a eu une actualité concernant chaque entreprise (colonne `X_ACTU`) ou non (valeur \"NR\"). Lorsqu’une actualité est présente, elle est décrite sous forme de titre. En parallèle, chaque ligne fournit le rendement boursier à 5 jours ouvrés pour chaque entreprise (colonne `X_RDMT_5`). Ainsi, les données permettent d’étudier l’impact potentiel d’une actualité sur le rendement futur d’un actif, en combinant texte (actualités) et données financières (rendements)."
      ],
      "metadata": {
        "id": "jpeDzpxFUx6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_csv('actu_finance.csv')\n",
        "data.head()\n",
        "data.describe()\n"
      ],
      "metadata": {
        "id": "OZgSD0kxAU2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On va mettre en index les dates et les classer par ordre croissant.\n"
      ],
      "metadata": {
        "id": "Tf7aqHCFU9gE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['Unnamed: 0']=pd.to_datetime(data['Unnamed: 0'])\n",
        "data.sort_values('Unnamed: 0')\n",
        "data.set_index('Unnamed: 0',inplace=True)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "q-5NsPjgAt_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Statistique descriptive\n",
        "\n",
        "On s'occupe ici d'effectuer les statistiques descriptives pour notre dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "fmuzetX1BaD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "hGgBGW-nBYT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le dataset ne contient aucune valeur manquantes. On va maintenat remplacer la valeur \"NR\" par \"nan\" et afficher le pourcentage d'actualité disponible pour chaque actif et on affiche des statistiques descriptives génerales."
      ],
      "metadata": {
        "id": "BPMc93NiWzJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.replace(\"NR\", np.nan, inplace=True)\n",
        "print(\"Pourcentages indisponibles par colones\")\n",
        "\n",
        "print(100*data[[col for col in data.columns if col.endswith('_ACTU')]].isna().sum()/data.shape[0])\n",
        "\n",
        "print(\"Nombre d'actu indisponibles au total\")\n",
        "print(data.isna().sum().sum())\n",
        "\n",
        "\n",
        "print(\"Résumé statistique :\")\n",
        "display(data.describe(exclude=[object]))\n",
        "data.describe(include=[object])\n"
      ],
      "metadata": {
        "id": "BmeKpxHdBrlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ici on sépare notre dataframe d'origine en dataframe pour chaque actif avec l'actualité et le rendement correspondant."
      ],
      "metadata": {
        "id": "aTrJU0bZXUN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dic={}\n",
        "# for col in data.columns[:8]:\n",
        "#   if col.endswith('_ACTU'):\n",
        "#     dic[f\"{col.split('_')[0]}\"]=data[[f\"{col}\", f\"{col.split('_')[0]}_RDMT_5\"]].dropna()\n",
        "\n",
        "dic={}\n",
        "for col in data.columns[:8]:\n",
        "  if col.endswith('_ACTU'):\n",
        "    dic[f\"{col.split('_')[0]}\"]=data[[f\"{col}\", f\"{col.split('_')[0]}_RDMT_5\"]].dropna()\n",
        "    dic[f\"{col.split('_')[0]}\"].columns=['ACTU', 'RDMT']\n",
        "statistique={}\n",
        "# for key, value in dic.items():\n",
        "#   print('Statistiques descriptives pour ',key)\n",
        "#   print(value.describe(include=[object]))\n",
        "#   print(value.describe(exclude=[object]))\n",
        "#   print('-----------------------------------------------------------------------'*2)\n",
        "#   print('\\n')\n"
      ],
      "metadata": {
        "id": "vwzK8dZJDRFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On affiche ensuite le nombre d'actualité par année"
      ],
      "metadata": {
        "id": "R_i9mxA-YLb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#dic['BNP']\n",
        "# rest=pd.concat([dic['BNP'], dic['AIR']])\n",
        "# rest.dropna().head()\n",
        "\n",
        "# dic['BNP'].iloc[:,1:2].groupby(dic['BNP'].index.year).count()\n",
        "\n",
        "# nbr_actu={}\n",
        "# for key,value in dic.items():\n",
        "#   print(\"Nombre d'actualité par année\")\n",
        "#   print(value.iloc[:,1:2].groupby(value.index.year).count())\n",
        "#   print('-'*100)\n",
        "#   print('\\n')\n",
        "\n",
        "\n",
        "nbr_actu = {}\n",
        "\n",
        "for key, value in dic.items():\n",
        "    # On extrait le nombre d'actualités par année\n",
        "    counts_by_year = value.iloc[:, 1:2].groupby(value.index.year).count()\n",
        "    counts_by_year.columns = ['Nombre d\\'actualités']  # Optionnel : renommer la colonne\n",
        "\n",
        "    nbr_actu[key] = counts_by_year\n",
        "\n",
        "# Fusion de tous les DataFrames en un seul (colonnes multi-indexées par clé)\n",
        "df_result = pd.concat(nbr_actu, axis=1)\n",
        "\n",
        "# Affichage\n",
        "print(\"Nombre d'actualités par année pour chaque catégorie :\")\n",
        "display(df_result)\n"
      ],
      "metadata": {
        "id": "mQeM0m_vExmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On affiche la correlation"
      ],
      "metadata": {
        "id": "pT0Pw7zxbWWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Select only numerical columns\n",
        "numerical_cols = data[[x for x in data.columns if x.endswith(\"_RDMT_5\")]]\n",
        "\n",
        "# Compute the correlation matrix\n",
        "corr_matrix = numerical_cols.corr()\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True)\n",
        "plt.title(\"Correlation Heatmap for Numerical Columns\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oY4c6ay8ben3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On va afficher un graph mentrant le rendement"
      ],
      "metadata": {
        "id": "65ZQzceIc9Mp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialisation du graphique\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.title(\"Distribution des rendements à 5 jours (RDMT_5) par actif\", fontsize=16)\n",
        "plt.xlabel(\"Rendement à 5 jours (%)\", fontsize=12)\n",
        "plt.ylabel(\"Densité\", fontsize=12)\n",
        "\n",
        "# Tracer la courbe KDE pour chaque actif\n",
        "for key, actif in dic.items():\n",
        "    sns.kdeplot(data=actif, x=\"RDMT\", label=key, fill=True, alpha=0.3)\n",
        "\n",
        "\n",
        "# Ajout de la légende\n",
        "plt.legend(title=\"Actifs\", fontsize=10, title_fontsize=12)\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Boucle sur chaque actif dans le dictionnaire\n",
        "for key, df in dic.items():\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    sns.kdeplot(data=df, x=\"RDMT\", fill=True, alpha=0.3, color='steelblue')\n",
        "\n",
        "    plt.title(f\"Distribution du rendement à 5 jours pour l'actif {key}\", fontsize=15)\n",
        "    plt.xlabel(\"Rendement à 5 jours (%)\", fontsize=12)\n",
        "    plt.ylabel(\"Densité\", fontsize=12)\n",
        "    plt.grid(True, linestyle='--', alpha=0.5)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "4-W-s9vhc8UQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour la suite on va retenir 5 actifs :"
      ],
      "metadata": {
        "id": "a7DMZNCTcz_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Liste des actifs choisis — correction des guillemets\n",
        "choosen_actif = [\"AIR\", \"ORA\", \"ALO\", \"CAP\", \"GLE\"]\n",
        "\n",
        "# Filtrage du dictionnaire avec les actifs choisis\n",
        "tab = [value for key, value in dic.items() if key in choosen_actif]\n",
        "\n",
        "# Concaténation des DataFrames sélectionnés\n",
        "df = pd.concat(tab, axis=0).reset_index()\n",
        "\n",
        "# Renommage de la colonne index\n",
        "df.rename(columns={'Unnamed: 0': 'Date'}, inplace=True)\n",
        "\n",
        "# Définition de l'index temporel\n",
        "df.set_index('Date', inplace=True)\n",
        "\n",
        "# Affichage des premières lignes\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "e-xZWfbcPik0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On va calculer le quantile à 20% de la variable RDMT_5 que l'on notera « q20 »\n",
        "A partir de la variable RDMT_5 construire la variable Y telle que:\n",
        "- Y = 1 lorsque RDMT_5 < q20\n",
        "- Y = 0 lorsque RDMT_5 ≥ q20"
      ],
      "metadata": {
        "id": "lpODeyYfaoLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q20=df['RDMT'].quantile(0.2)\n",
        "print(q20)\n",
        "df['Y']=df['RDMT'].apply(lambda x: 1 if x<q20 else 0)\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "D8JaECGdB5Qu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip uninstall torch torchvision torchaudio -y\n",
        "#!pip install torch torchvision torchaudio\n"
      ],
      "metadata": {
        "id": "iKhRgQbdh30-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -U sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Chargement du modèle\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Encodage de tous les textes en une seule fois (vectorisé)\n",
        "embeddings = model.encode(df['ACTU'].astype(str).tolist(), show_progress_bar=True)\n",
        "\n",
        "# Transformation en DataFrame\n",
        "embeddings_df = pd.DataFrame(embeddings, columns=[f'emb_{i}' for i in range(embeddings.shape[1])])\n",
        "embeddings_df.set_index(df.index, inplace=True)\n",
        "\n",
        "# Fusion avec le DataFrame initial\n",
        "df_encoded = pd.concat([df, embeddings_df], axis=1)\n",
        "\n",
        "\n",
        "\n",
        "# Visualisation\n",
        "df_encoded.head()"
      ],
      "metadata": {
        "id": "QmwHSXb4DTl5",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_encoded.head()"
      ],
      "metadata": {
        "id": "UCyVbK8SmzVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Extraction des features X et de la cible Y\n",
        "\n",
        "df_train = df_encoded[df_encoded.index.year < 2023]\n",
        "df_test = df_encoded[df_encoded.index.year >= 2023]\n",
        "\n",
        "X_train = df_train.filter(like='emb_')\n",
        "X_test =  df_test.filter(like='emb_')\n",
        "y_train = df_train['Y']\n",
        "y_test =  df_test['Y']\n",
        "\n"
      ],
      "metadata": {
        "id": "hrweQI35HId7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural network"
      ],
      "metadata": {
        "id": "lwzhzwOoHm2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dropout(0.2),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # binaire\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)"
      ],
      "metadata": {
        "id": "kglB7zFWHpEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "y_pred_prob = model.predict(X_test)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "print(\"F1 Score :\", f1_score(y_test, y_pred))\n",
        "print(\"Précision :\", precision_score(y_test, y_pred))\n",
        "print(\"Recall :\", recall_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "DBU4Itt-Is3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_encoded.to_csv('actu_finance_emb.csv')"
      ],
      "metadata": {
        "id": "BnxrB_nJI-du"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelisation"
      ],
      "metadata": {
        "id": "J4cDvzWapZQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikeras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n"
      ],
      "metadata": {
        "id": "4w16KuGsqr-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikeras"
      ],
      "metadata": {
        "id": "G1o2TnO6sO5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "# 1️⃣ Fonction pour construire le modèle\n",
        "def build_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, activation='relu', input_dim=384))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=[tf.metrics.F1Score(average='micro', threshold=0.5),\n",
        "                  \"recall\",\n",
        "                  \"precision\"\n",
        "                  ])\n",
        "    return model\n",
        "\n",
        "# 2️⃣ Création du pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),  # Normalisation des embeddings\n",
        "    ('clf', KerasClassifier(model=build_model, epochs=20, batch_size=32, verbose=1))\n",
        "])\n"
      ],
      "metadata": {
        "id": "fHFx3qahquwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Découpage et apprentissage"
      ],
      "metadata": {
        "id": "_V6PSA6zryJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ],
      "metadata": {
        "id": "JpIlsV0Iu65C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Assure-toi que 'Date' est l'index\n",
        "df = df_encoded.copy()\n",
        "df.index = pd.to_datetime(df.index)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "W_cAtbt31sve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ▪️ Découpe temporelle : train < 2023 / test = 2023\n",
        "train_df = df[df.index.year < 2023]\n",
        "test_df = df[df.index.year >= 2023]\n",
        "\n",
        "# ▪️ Données d'entrée / sortie\n",
        "X_train = train_df.filter(like='emb_')\n",
        "y_train = train_df['Y']\n",
        "X_test = test_df.filter(like='emb_')\n",
        "y_test = test_df['Y']\n",
        "X_train.head()\n",
        "\n",
        "y_train.value_counts()"
      ],
      "metadata": {
        "id": "3fLzBhSs14PK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# ▪️ Création du MLP\n",
        "model = Sequential()\n",
        "model.add(Dense(128, activation='relu', input_dim=X_train.shape[1]))\n",
        "#model.add(Dropout(0.3))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "#model.add(Dropout(0.3))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# ▪️ Compilation\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[ \"accuracy\",\"recall\",\n",
        "          \"precision\"])\n",
        "\n",
        "# ▪️ Entraînement avec EarlyStopping\n",
        "model.fit(X_train, y_train, validation_split=0.2, epochs=3, batch_size=32)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s7OBS4Kx1JWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ▪️ Prédiction sur le test\n",
        "y_pred_prob = model.predict(X_test)\n",
        "y_pred = (y_pred_prob >= 0.5).astype(int)\n",
        "\n",
        "\n",
        "# ▪️ Rapport de classification\n",
        "print(classification_report(y_test, y_pred, digits=4))"
      ],
      "metadata": {
        "id": "FHfi-MUq3s0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "print(\"F1 Score    :\", f1_score(y_test, y_pred))\n",
        "print(\"Précision   :\", precision_score(y_test, y_pred))\n",
        "print(\"Recall      :\", recall_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "PwqNXsCH4bxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['Y'].value_counts()"
      ],
      "metadata": {
        "id": "V-ekp0I35PVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import class_weight\n",
        "import numpy as np\n",
        "\n",
        "# Classe pondérée automatiquement\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "class_weights = dict(enumerate(class_weights))\n",
        "print(\"Class weights :\", class_weights)\n"
      ],
      "metadata": {
        "id": "5kZSQqoQ5ulH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Standardisation\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Modèle\n",
        "model = Sequential()\n",
        "model.add(Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "def f1_metric(y_true, y_pred):\n",
        "    # Convertir y_true et y_pred au même type\n",
        "    y_true = K.cast(y_true, 'float32')\n",
        "    y_pred = K.round(y_pred)\n",
        "\n",
        "    tp = K.sum(y_true * y_pred)\n",
        "    fp = K.sum((1 - y_true) * y_pred)\n",
        "    fn = K.sum(y_true * (1 - y_pred))\n",
        "\n",
        "    precision = tp / (tp + fp + K.epsilon())\n",
        "    recall = tp / (tp + fn + K.epsilon())\n",
        "\n",
        "    f1 = 2 * precision * recall / (precision + recall + K.epsilon())\n",
        "    return 0.5-f1\n",
        "\n",
        "\n",
        "# Compilation avec F1 comme métrique\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[f1_metric])\n",
        "\n",
        "# Early stopping\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Entraînement\n",
        "model.fit(X_train_scaled, y_train,\n",
        "          validation_split=0.2,\n",
        "          epochs=30,\n",
        "          batch_size=32,\n",
        "          class_weight=class_weights,\n",
        "          verbose=1)\n"
      ],
      "metadata": {
        "id": "Tj8FgZrk5yXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "# Prédictions probabilistes\n",
        "y_pred_prob = model.predict(X_test_scaled)\n",
        "\n",
        "# Optimisation du seuil\n",
        "thresholds = np.linspace(0.1, 0.9, 17)\n",
        "best_thresh, best_f1 = 0.5, 0\n",
        "\n",
        "for t in thresholds:\n",
        "    y_pred = (y_pred_prob >= t).astype(int)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_thresh = t\n",
        "\n",
        "print(f\"✅ Meilleur seuil : {best_thresh:.2f}, F1 : {best_f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "IDKu7BAj5yxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_final = (y_pred_prob >= best_thresh).astype(int)\n",
        "\n",
        "print(\"Classification Report :\")\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, y_final, digits=4))\n"
      ],
      "metadata": {
        "id": "DeZhhkUX512o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "print(\"F1 Score    :\", f1_score(y_test, y_pred))\n",
        "print(\"Précision   :\", precision_score(y_test, y_pred))\n",
        "print(\"Recall      :\", recall_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "4vtF15kW8vzw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}